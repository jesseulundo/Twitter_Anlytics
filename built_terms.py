import operator 
import sqlite3
from nltk import *
from nltk.corpus import knbc
from nltk.corpus import jeita
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import zlib
import time
import nltk.tokenize.api
import tinysegmenter
import re
import math
from textblob import TextBlob as tb

conn = sqlite3.connect('streaming_data.sqlite')
cur = conn.cursor()

emoticons_str = r"""
	(?:
		[:=;] # Eyes
		[oO\-]? # Nose (optional)
		[D\)\]\(\]/\\OpP] # Mouth
	)"""
regex_str = [
    emoticons_str,
    r'<[^>]+>', # HTML tags
    r'(?:@[\w_]+)', # @-mentions
    r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # hash-tags
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
 
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # numbers
    r"(?:[a-z][a-z'\-_]+[a-z])", # words with - and '
    r'(?:[\w_]+)', # other words
    r'(?:\S)' # anything else
]
tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)

def tokenize(s):
	return tokens_re.findall(s)
def preprocess(s, lowercase=False):
	tokens = tokenize(s)
	if lowercase:
		tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
	return tokens
def tf(word, blob):
	return blob.words.count(word)/len(blob.words)
def n_containing(word, bloblist):
	return sum(1 for blob in bloblist if word in blob.words)
def idf(word, bloblist):
	return math.log(len(bloblist)/(1+n_containing(word, bloblist)))
def tfidf(word, blob, bloblist):
	return tf(word, blob) * idf(word, bloblist)



segmenter = tinysegmenter.TinySegmenter()
count_all = Counter()

def j_stopwords():
	japanese_stopwords = ["ã“ã‚Œ", "ãã‚Œ", "ã‚ã‚Œ", "ã“ã®", "ãã®", "ã‚ã®", "ã“ã“", "ãã“", "ã‚ãã“", "ã“ã¡ã‚‰",
						"ã©ã“", "ã ã‚Œ", "ãªã«", "ãªã‚“", "ä½•", "ç§", "è²´æ–¹", "è²´æ–¹æ–¹", "æˆ‘ã€…", "ç§é”",
						"ã‚ã®äºº", "ã‚ã®ã‹ãŸ", "å½¼å¥³", "å½¼", "ã§ã™", "ã‚ã‚Šã¾ã™", "ãŠã‚Šã¾ã™", "ã„ã¾ã™", "ã¯", 
						"ãŒ", "ã®", "ã«", "ã‚’", "ã§", "ãˆ", "ã‹ã‚‰", "ã¾ã§", "ã‚ˆã‚Š", "ã‚‚", "ã©ã®", "ã¨", "ã—",
						"ãã‚Œã§", "ã—ã‹ã—", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l",
						"m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", ".", 
						",", '"', "'", "/", "|", "?", "!", ";", "*", ":", "+", '\\', "_", "ã®", "ã«",
						"ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•", "ã‚ã‚‹"," ã„ã‚‹", "ã‚‚", "ã™ã‚‹", 
						"ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„", "ã‚Œã‚‹", "ãªã©", "ãªã£", "ãªã„", "ã“ã®", "ãŸã‚", 
						"ãã®", "ã‚ã£", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®", "ã¨ã„ã†", "ã‚ã‚Š", "ã¾ã§", "ã‚‰ã‚Œ", "ãªã‚‹", "ã¸", "ã‹", 
						"ã ", "ã“ã‚Œ", "ã«ã‚ˆã£ã¦", "ã«ã‚ˆã‚Š", "ãŠã‚Š", "ã‚ˆã‚Š", "ã«ã‚ˆã‚‹", "ãš", "ãªã‚Š", "ã‚‰ã‚Œã‚‹", "ã«ãŠã„ã¦",
						"ã°", "ãªã‹ã£", "ãªã", "ã—ã‹ã—", "ã«ã¤ã„ã¦", "ã›", "ã ã£", "ãã®å¾Œ", "ã§ãã‚‹", "ãã‚Œ", "ã†", 
						"ã®ã§", "ãªãŠ", "ã®ã¿", "ã§ã", "ã", "ã¤", "ã«ãŠã‘ã‚‹", "ãŠã‚ˆã³", "ã„ã†", "ã•ã‚‰ã«", "ã§ã‚‚", 
						"ã‚‰", "ãŸã‚Š", "ãã®ä»–", "ã«é–¢ã™ã‚‹", "ãŸã¡", "ã¾ã™", "ã‚“", "ãªã‚‰", "ã«å¯¾ã—ã¦", "ç‰¹ã«", "ã›ã‚‹", "åŠã³", 
						"ã“ã‚Œã‚‰", "ã¨ã", "ã§ã¯", "ã«ã¦", "ã»ã‹", "ãªãŒã‚‰", "ã†ã¡", "ãã—ã¦", "ã¨ã¨ã‚‚ã«", "ãŸã ã—", "ã‹ã¤ã¦", 
						"ãã‚Œãã‚Œ", "ã¾ãŸã¯", "ãŠ", "ã»ã©", "ã‚‚ã®ã®", "ã«å¯¾ã™ã‚‹", "ã»ã¨ã‚“ã©", "ã¨å…±ã«", "ã¨ã„ã£ãŸ", "ã§ã™", 
						"ã¨ã‚‚", "ã¨ã“ã‚", "ã“ã“", "ã‚ã£", "ã‚ã‚Š", "ã‚ã‚‹", "ã„", "ã„ã†", "ã„ã‚‹", "ã†", "ã†ã¡", "ãŠ", "ãŠã‚ˆã³",
						"ãŠã‚Š", "ã‹", "ã‹ã¤ã¦", "ã‹ã‚‰", "ãŒ", "ã", "ã“ã“", "ã“ã¨", "ã“ã®", "ã“ã‚Œ", "ã“ã‚Œã‚‰", "ã•", "ã•ã‚‰ã«",
						"ã—", "ã—ã‹ã—", "ã™ã‚‹", "ãš", "ã›", "ã›ã‚‹", "ãã—ã¦", "ãã®", "ãã®ä»–", "ãã®å¾Œ", "ãã‚Œ","ï¸ï¸",
						"ãã‚Œãã‚Œ", "ãŸ", "ãŸã ã—", "ãŸã¡", "ãŸã‚", "ãŸã‚Š", "ã ", "ã ã£", "ã¤", "ã¦", "ã§", "ã§ã",'Ã—',
						"ã§ãã‚‹", "ã§ã™", "ã§ã¯", "ã§ã‚‚", "ã¨", "ã¨ã„ã†", "ã¨ã„ã£ãŸ", "ã¨ã", "ã¨ã“ã‚", "ã¨ã—ã¦", "ã¨ã¨ã‚‚ã«",
						"ã¨ã‚‚", "ã¨å…±ã«", "ãª", "ãªã„", "ãªãŠ", "ãªã‹ã£", "ãªãŒã‚‰", "ãªã", "ãªã£", "ãªã©", "ãªã‚‰",'â†’','ï¼',
						"ãªã‚Š", "ãªã‚‹", "ã«", "ã«ãŠã„ã¦", "ã«ãŠã‘ã‚‹", "ã«ã¤ã„ã¦", "ã«ã¦", "ã«ã‚ˆã£ã¦", "ã«ã‚ˆã‚Š","ã«ã‚ˆã‚‹",'ğŸ—»',
						"ã«å¯¾ã—ã¦", "ã«å¯¾ã™ã‚‹", "ã«é–¢ã™ã‚‹", "ã®", "ã®ã§", "ã®ã¿", "ã¯", "ã°", "ã¸", "ã»ã‹", "ã»ã¨ã‚“ã©",'Ã—',
						"ã»ã©", "ã¾ã™", "ã¾ãŸ", "ã¾ãŸã¯", "ã¾ã§", "ã‚‚", "ã‚‚ã®", "ã‚‚ã®ã®", "ã‚„", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚‰", "ã‚‰ã‚Œ",
						"ã‚‰ã‚Œã‚‹", "ã‚Œ", "ã‚Œã‚‹", "ã‚’", "ã‚“", "åŠã³", "ç‰¹ã«", "ã€", "ã€‚", "å¹´", "ã€Œ", "ã€", "æœˆ", "ã€", "ã€",
						"(", ")", "`", "``", "',", "'RT", "&",'\\n','\\u3000',',)','ã€‚\\','u3000','://','.co','T',
						"â€¦'",'ï¼\\','\\nhttps',',\\',"'ã€",'1','nhttps','ã€‘\\','ï¼ï¼','10','\\næ—¥æœ¬','ã€','0','ï¼Ÿ\\',
						'ã€\\','4','8','n2017','...','08',')\\','2','\\u3000ã™','.\\','\\u3000ã®','ï¼š','00','\\u3000ãª',
						'ï½£','ğŸ’—ğŸ’—','â™ª\\','ã€Œä¸¹','âœ¨\\','\\u3000ã‚Š','gt','\\u3000ãŠ','\\u3000ã„','â–·\\','\\nã„ã„','^Y','ã€\\',
						'18','5','ï¼‰\\','â†’\\','â€¦â€¦','ï¼ƒ',')\\n','\\u3000ã«','ï¼ˆ','ğŸ¦ğŸ’“','ğŸ¦â­','ï¸ğŸ¦','ğŸ’“\\n', '\\u3000 ã¾',"ã€‚'",
						'â€¦\\','(^','1960','â€¦','ï¼','ã€‘','ãƒ»','ã•ã‚“','ï¼‰','ï¼Ÿ','â”€','ãƒ½','â”','ã€œ','â”ƒ','ï½','ğŸŒ¸','ï¼¼','ã‚','ï¸ï¸','__',
						'ã„','ã†','ãˆ','ãŠ','ã‹','ã','ã','ã‘','ã“','ãŸ','ã¡','ã¤','ã¦','ã¨','ã•','ã—','ã™','ã›','ã','ãŒ','ã','ã',
						'ã’','ã”','ã–','ã˜','ãš','ãœ','ã','ã ','ã¢','ã¥','ã§','ã©','ãª','ã«','ã¬','ã­','ã®','ã¯','ã²','ãµ','ã¸','ã»',
						'ã°','ã³','ã¶','ã¹','ã¼','ã±','ã´','ã·','ãº','ã½','ã¾','ã¿','ã‚€','ã‚','ã‚‚','ã‚„','ã‚†','ã‚ˆ','ã‚‰','ã‚Š','ã‚‹','ã‚Œ',
						'ã‚','ã‚','ã‚’','ã‚“', 'RT']
	return japanese_stopwords
result = cur.execute("SELECT message FROM tweets WHERE date BETWEEN 'Fri Sep 08 00:00:00 +0000 2017' AND 'Fri Sep 08 23:59:59 +0000 2017'")
punctuation = list(string.punctuation)
stop = stopwords.words('english') + punctuation +j_stopwords()
for lines in result:
	segments = [segmenter.tokenize(x) for x in preprocess(lines[0])]
	for i, s in enumerate(segments):
		if "https" in s:
			segments[i] = ["".join(s)]
	
	#print(segmenter.tokenize(lines[0]))
	#terms_all = [term for term in segmenter.tokenize(preprocess(str(lines))) if term not in stop]
	segments = [item for segment in segments for item in segment]
	#print(segments)
	terms_all = [term for term in segments if term not in stop]
	terms_all = [term for term in terms_all if not term.isdigit()]

	count_all.update(terms_all)
	terms_single = set(terms_all)	

most_common = count_all.most_common(300)
sorted(most_common,key=lambda x:x[1])
most_common = dict(most_common)
print(most_common)

for c, vl in most_common.items():
	cur.execute('''insert or ignore into term_frequency (term, amount)
	values (?, ?)''', (c, vl))
	
x = sorted(most_common, key=most_common.get, reverse=True)
highest = None
lowest = None
for k in x[:100]:
	if highest is None or highest < most_common[k]:
		highest = most_common[k]
	if lowest is None or lowest > most_common[k]:
		lowest = most_common[k]
print('Range of most_common:',highest,lowest)

bigsize = 80
smallsize = 20

fhand = open('twtword1.js','w', encoding='utf-8')
fhand.write("twtword1 = [")
first = True
for k in x[:100]:
	if not first : fhand.write( ",\n")
	first = False
	size = most_common[k]
	size = (size - lowest) / float(highest - lowest)
	size = int((size * bigsize) + smallsize)
	fhand.write("{text: '"+k+"', size: "+str(size)+"}")
fhand.write("\n];\n")

print("Output written to twtword1.js")
print("Open twtword.htm in a browser to see the vizualization")

conn.commit()	
conn.close()